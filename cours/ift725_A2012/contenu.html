<!DOCTYPE html >
<html >
  <head>
    <title>Hugo Larochelle</title>
    <meta http-equiv="content-type" content="text/html; charset=iso-8859-1" />
    <link rel="stylesheet" href="../../css/1.css" type="text/css" media="screen,projection" />

  </head>
  
  <body>

    <div id="wrapper">
      <div id="innerwrapper">

	<div id="header">
	  <font color="#FFFFFF">
	    <font face="Copperplate" size=8>Hugo Larochelle</font>
	    <br> http://www.dmi.usherb.ca/~larocheh/
	  </font>
	  <!-- <h1 id="header-right"></h1>-->
	    <ul id="nav">
	      
	      <li><a href="../../index_fr.html" accesskey="a"><em>A</em>ccueil</a></li>
	      
	      <li><a href="../../publications_fr.html" accesskey="p"><em>P</em>ublications</a></li>

	      <li><a href="../../university_fr.html" accesskey="u" class="active"><em>U</em>niversité</a></li>

	      <li><a href="../../links_fr.html" accesskey="l"><em>L</em>iens</a></li>

	    </ul>
	  </div>
	  
	  <div id="sidebar">
	    <h2>IFT 725 <small>(Automne 2012)</small></h2>
	    <table id="nav" cellpadding="0" cellspacing="2" >
	      <tr><td><a href="description.html">Description</a></td></tr>
	      <tr><td class="active"><a href="contenu.html">Contenu</a></td></tr>
	      <tr><td><a href="evaluations.html">Évaluations</a></td></tr>
	    </table>
	    <h2>Annonces</h2>
	    <b>[03/12/2012]</b><br>
	    Les notes des présentations sont disponibles.<br><br>

	    <b>[25/11/2012]</b><br>
	    Les notes du devoir 3 sont disponibles.<br><br>

	    <b>[29/10/2012]</b><br>
	    Les notes du devoir 2 sont disponibles.<br><br>

	    <b>[12/10/2012]</b><br>
	    Le devoir 3 est disponible.<br><br>

	    <b>[07/10/2012]</b><br>
	    Les notes du devoir 1 sont disponibles.<br><br>

	    <b>[21/09/2012]</b><br>
	    Le devoir 2 est disponible.<br><br>

        <b>[31/08/2012]</b><br>
        Les prochains cours auront lieu
        les vendredis, de 8h30 à 11h30, au D4-1023<br><br>

	    <b>[28/08/2012]</b><br>
	    Premier cours: <br>31 août, 9h-12h, au D4-2024.<br><br>

	    <b>[27/08/2012]</b><br>
	    Le devoir 1 est disponible.<br><br>

	    <b>[20/08/2012]</b><br>
	    La séance d'information sera
	    le <b>27 août, à 13h00, au D4-2025.</b><br><br>

	    <b>[13/08/2012]</b><br>
	    Bienvenue au cours IFT 725!<br>
	  </div>
	  
	  <!--
	  <div id="sidebarright">
	  </div>
	  -->

	  <div id="contentnorightbar">
	    <h2> Contenu du cours </h2>
	    
	    <p>
	      Pour toute question concernant le contenu, n'hésitez pas à poser une question
	      sur le <a href="https://groups.google.com/forum/?fromgroups#!forum/ift-725-a2012">forum de discussion</a> du cours!
	    </p>

        <p>
          Considérez également vous inscrire au
          cours <a href="https://www.coursera.org/course/neuralnets">Neural
          Networks for Machine Learning</a>, donné
          par <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey
          Hinton</a> et offert
          par <a href="https://www.coursera.org">Coursera</a>.
        </p>

	    <table cellspacing="0">
	      <tr bgcolor="#666">
		<td width="11%"><font color="#FFF"><b>Date</b></font></td> 
		<td ><font color="#FFF"><b>Contenu du cours</b></font></td> 
	      
	      <tr>
		<td>VE 31/08</td> 
		<td>
		  <b><font face="Copperplate,georgia,Verdana">Introduction et révision mathématique</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_mise_a_niveau.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/review.pdf">pdf</a>] [<a href="diapositives/review/index.html">html</a>]
		  <ul>
		    <li> <b>Mise à niveau apprentissage automatique:</b> pages 1 à 16 et 27 à 33 de <a href="http://www.dmi.usherb.ca/~larocheh/publications/thesis.pdf">ma thèse de doctorat</a>
		    <li> <b>Mise à niveau en calcul différentiel:</b> pages 28 à 46 et 61 à 62 des 
		      <a href="diapositives/ift615-apprentissage-automatique.pdf">diapositives</a> sur l'apprentissage automatique du cours IFT 615
		    <li> <b>Mise à niveau en algèbre linéaire:</b> <a href="http://see.stanford.edu/materials/aimlcs229/cs229-linalg.pdf">révision</a> du cours d'<a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
		    <li> <b>Mise à niveau en probabilités:</b> <a href="http://see.stanford.edu/materials/aimlcs229/cs229-prob.pdf">révision</a> du cours d'<a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
		    <li> <b>Mise à niveau en statistiques:</b> chapitre 3 des <a href="http://math.arizona.edu/~faris/stat.pdf">notes de cours</a> de <a href="http://math.arizona.edu/~faris/">William Faris</a>
		    <li> <b>Mise à niveau en échantillonnage:</b> Pages 20 à 31 de la <a href="http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf">thèse de doctorat</a> 
		      de <a href="http://homepages.inf.ed.ac.uk/imurray2/">Iain Murray</a>
		    <li> <b>Mise à niveau Python:</b> <a href="http://www.dmi.usherb.ca/~larocheh/cours/tutoriel_python.html">tutoriel Python</a>
		    <li> <b><a href="http://www.dmi.usherb.ca/~larocheh/mlpython/">MLPython</a></b>: <a href="http://www.dmi.usherb.ca/~larocheh/mlpython/tutorial.html#tutorial">tutoriel</a> sur la librairie d'apprentissage automatique
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> <a href="http://web.mit.edu/~wingated/www/stuff_i_use/matrix_cookbook.pdf"><i>The Matrix Cookbook</i></a> 
		      de Kaare Brandt Petersen et Michael Syskind Pedersen (excellente référence, à consulter régulièrement)
		    <li> Sections 7.1, 7.2, 7.3 et 7.10 du livre <a href="http://www.stanford.edu/~hastie/local.ftp/Springer/ESLII_print5.pdf"><i>The Elements of Statistical Learning</i></a> de <a href="http://www.stanford.edu/~hastie/">Trevor Hastie</a>, <a href="http://www-stat.stanford.edu/~tibs/">Robert Tibshirani</a> et <a href="http://www-stat.stanford.edu/~jhf/">Jerome Friedman</a>
		    <li> Sections 18.2.1, 18.2.2 et 18.3 du livre <a href="http://www.naturalimagestatistics.net/nis_preprintFeb2009.pdf"><i>Natural Image Statistics</i></a>  de <a href="http://www.cs.helsinki.fi/u/ahyvarin/">Aapo Hyvärinen</a>, Jarmo Hurri et <a href="http://www.cs.helsinki.fi/u/phoyer/">Patrik Hoyer</a>
		    <li> Section 19 du livre <a href="http://www.naturalimagestatistics.net/nis_preprintFeb2009.pdf"><i>Natural Image Statistics</i></a>
		    <li> <a href="diapositives/probx.pdf">Diapositives de révision</a> par <a href="http://www.cs.nyu.edu/~roweis/">Sam Roweis</a>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Vidéos du cours <i>Machine Learning</i> de 
		      <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> 
		      sur l'apprentissage automatique
		      [<a href="http://www.youtube.com/watch?v=HN7VK4aDFiA&feature=relmfu">1</a>] 
		      [<a href="http://www.youtube.com/watch?v=RxhXtz6rCBc&feature=relmfu">2</a>] 
		      [<a href="http://www.youtube.com/watch?v=l9kvAXe5lVA&feature=relmfu">3</a>] 
		      [<a href="http://www.youtube.com/watch?v=7ebYohehD1g&feature=relmfu">4</a>] 
		      [<a href="http://www.youtube.com/watch?v=_Je5f750bp4&feature=relmfu">5</a>] 
		      [<a href="http://www.youtube.com/watch?v=fgmMm-nWN1s&feature=relmfu">6</a>] 
		      [<a href="http://www.youtube.com/watch?v=ZrIUWY3tkVU&feature=relmfu">7</a>] 
		      [<a href="http://www.youtube.com/watch?v=as2z9I41Db8&feature=relmfu">8</a>] 
		      [<a href="http://www.youtube.com/watch?v=0IYePTE2ZVw&feature=relmfu">9</a>] 
		      [<a href="http://www.youtube.com/watch?v=o-HVrPZhd70&feature=relmfu">10</a>] 
		      [<a href="http://www.youtube.com/watch?v=Tp7XNqCn1Vs&feature=relmfu">11</a>] 
		      [<a href="http://www.youtube.com/watch?v=c2yKtYEfv10&feature=relmfu">12</a>] 

		    <li> Vidéos du cours <i>Machine Learning</i> de 
		      <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> 
		      sur l'algèbre linéaire
		      [<a href="http://www.youtube.com/watch?v=PsWi-Mgxftc&feature=relmfu">1</a>] 
		      [<a href="http://www.youtube.com/watch?v=gdToHE-oCxQ&feature=relmfu">2</a>] 
		      [<a href="http://www.youtube.com/watch?v=9zOZCVj-0Kg&feature=relmfu">3</a>] 
		      [<a href="http://www.youtube.com/watch?v=w3wyOBSLf8s&feature=relmfu">4</a>] 
		      [<a href="http://www.youtube.com/watch?v=qI_2jC4vK3g&feature=relmfu">5</a>] 

		    <li> Présentation <a href="http://videolectures.net/mlss2010_lawrence_mlfcs/"><i>What is Machine Learning</i></a> 
		      de <a href="http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/">Neil Lawrence</a>
		    <li> Présentation <a href="http://videolectures.net/mlss09uk_murray_mcmc/"><i>Markov Chain Monte Carlo</i></a> 
		      de <a href="http://homepages.inf.ed.ac.uk/imurray2/">Iain Murray</a>
		  </ul>
		</td> 

	      <tr bgcolor="#EEE">
		<td>VE 07/09</td> 
		<td>
		  <b><font face="Copperplate,georgia,Verdana">Réseau de neurones à propagation avant</font></b><br>
		  <br><b>- Lectures obligatoires -</b>  [<a href="questions_propagation_avant.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/fprop.pdf">pdf</a>] [<a href="diapositives/fprop/index.html">html</a>]
		  <ul>
		    <li> Section 1, 2 et 3 de la <a href="diapositives/adam_salvail_reseaux_de_neurones.pdf">description des réseaux de neurones</a> par Adam Salvail-Bérard
		    <li> Section 11.3 du livre <a href="http://www.stanford.edu/~hastie/local.ftp/Springer/ESLII_print5.pdf"><i>The Elements of Statistical Learning</i></a>
		    <li> Pages 16 à 23 de <a href="http://www.dmi.usherb.ca/~larocheh/publications/thesis.pdf">ma thèse de doctorat</a>
		    <li> Pages 1 à 7 des <a href="http://www.iro.umontreal.ca/~vincentp/ift3395/cours/ReseauxDeNeurones.pdf">diapositives</a> du cours de <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a> sur les réseaux de neurones
		    <li> Section 3.1 du livre <a href="http://www.naturalimagestatistics.net/nis_preprintFeb2009.pdf"><i>Natural Image Statistics</i></a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul> 
		    <li> Articles développant le lien entre avec les réseaux de neurones biologiques pour développer de nouveaux réseaux artificiels:
		    <ul id="contenucours">
		      <li> <a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205"><i>Quadratic Polynomials Learn Better Image Features</i></a> de <a href="http://people.fas.harvard.edu/~bergstra/">James Bergstra</a>, <a href="http://brainlogging.wordpress.com/">Guillaume Desjardins</a>, Pascal Lamblin et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		      <li> <a href="http://jmlr.csail.mit.edu/proceedings/papers/v15/glorot11a/glorot11a.pdf"><i>Deep Sparse Rectier Neural Networks</i></a> de Xavier Glorot, <a href="https://www.hds.utc.fr/~bordesan/dokuwiki/doku.php">Antoine Bordes</a> et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		    </ul>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Vidéos du cours <i>Machine Learning</i> de 
		      <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> 
		      sur les réseaux de neurones 
		      [<a href="http://www.youtube.com/watch?v=DJB5N9sJx4Q">1</a>] 
		      [<a href="http://www.youtube.com/watch?v=Ob35PRyyGi4&feature=relmfu">2</a>] 
		      [<a href="http://www.youtube.com/watch?feature=endscreen&NR=1&v=GLOVoMGHmKA">3</a>] 
		      [<a href="http://www.youtube.com/watch?v=SIfr-sJw1DU&feature=relmfu">4</a>] 
		      [<a href="http://www.youtube.com/watch?v=bYYD854YaSU&feature=relmfu">5</a>] 
		      [<a href="http://www.youtube.com/watch?v=YiG7ptjPhfE&feature=relmfu">6</a>] 
		      [<a href="http://www.youtube.com/watch?v=gO-nONxbvUY&feature=relmfu">7</a>]
		  </ul>
		</td>
	      </tr>
	      
	      <tr>
		<td>VE 14/09</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Rétropropagation des gradients et optimisation</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_retropropagation.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/bprop.pdf">pdf</a>] [<a href="diapositives/bprop/index.html">html</a>]
		  <ul>
		    <li> Mise à niveau optimisation: section 18.6 du livre <a href="http://www.naturalimagestatistics.net/nis_preprintFeb2009.pdf"><i>Natural Image Statistics</i></a>
		    <li> Sections 4, 5 et 6 de la <a href="diapositives/adam_salvail_reseaux_de_neurones.pdf">description des réseaux de neurones</a> par Adam Salvail-Bérard
		    <li> Sections 11.4, 11.5 et 11.6 du livre <a href="http://www.stanford.edu/~hastie/local.ftp/Springer/ESLII_print5.pdf"><i>The Elements of Statistical Learning</i></a>
		    <li> <a href="http://www.iro.umontreal.ca/~pift6266/A06/cours/gradient.pdf">Notes de cours</a> de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> sur l'optimisation à base de gradient dans les réseaux de neurones
		    <li> Sections 2, 3.3.1 et 4.1 de <a href="http://arxiv.org/pdf/1206.5533v1.pdf"><i>Practical Recommendations for Gradient-Based Training of Deep
			Architectures</i></a> de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> 
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> Articles discutant plusieurs trucs pour entraîner des réeaux de neurones artificiels:
		      <ul id="contenucours">
			<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"><i>Efficient BackProp</i></a> de <a href="http://yann.lecun.com/">Yann LeCun</a>, <a href="http://leon.bottou.org/">Léon Bottou</a>, <a href="http://www.willamette.edu/~gorr/">Geneviève Orr</a> et <a href="http://www.ml.tu-berlin.de/menue/mitglieder/klaus-robert_mueller/">Klaus-Robert Müller</a>
			<li> <a href="http://arxiv.org/pdf/1206.5533v1.pdf"><i>Practical Recommendations for Gradient-Based Training of Deep
			      Architectures</i></a> de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> 
		      </ul>
		    <li> Articles explorant des techniques d'optimisation plus sophistiquées pour entraîner des réseaux de neurones:
		      <ul id="contenucours">
			<li> <a href="http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf "><i>Deep learning via Hessian-free optimization</i></a> de <a href="http://www.cs.toronto.edu/~jmartens/research.html">James Martens</a>
			<li> <a href="http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf"><i>Fast Exact Multiplication by the Hessian</i></a> de <a href="http://www.bcl.hamilton.ie/~barak/">Barak Pearlmutter</a>
			<li> <a href="http://nicolas.le-roux.name/publications/LeRoux08_tonga.pdf">Topmoumoute online natural gradient algorithm</a> de <a href="http://nicolas.le-roux.name/">Nicolas Le Roux</a>, Pierre-Antoine Manzagol et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		      </ul>
		    <li> Notes générales sur l'optimisation sur de gros jeux de données (un excellent résumé de plusieurs méthodes):
		      <ul id="contenucours">
			<li> <a href="http://www.di.ens.fr/~mschmidt/Documents/bigN.pdf"><i>Notes on Big-n Problems</i></a> de <a href="http://www.di.ens.fr/~mschmidt/">Mark Schmidt</a>
		      </ul>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Vidéos du cours <i>Machine Learning</i> de 
		      <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> 
		      sur l'entraînement de réseaux de neurones 
		      [<a href="http://www.youtube.com/watch?v=keQ1kNIU96Y&feature=relmfu">1</a>] 
		      [<a href="http://www.youtube.com/watch?v=wmfpS5fKFeY&feature=relmfu">2</a>] 
		      [<a href="http://www.youtube.com/watch?v=b0mv1sJvRp0">3</a>] 
		      [<a href="http://www.youtube.com/watch?v=12a9fsLyFes&feature=relmfu">4</a>] 
		      [<a href="http://www.youtube.com/watch?v=feEj-T2Ceg4&feature=relmfu">5</a>] 
		      [<a href="http://www.youtube.com/watch?feature=endscreen&NR=1&v=I_TeNU-nUQs">6</a>] 
		      [<a href="http://www.youtube.com/watch?v=VwwB6xcx8Wg&feature=relmfu">7</a>] 
		  </ul>
		</td> 

	      <tr bgcolor="#EEE">
		<td>VE 21/09</td> 
		<td>
		  <b><font face="Copperplate,georgia,Verdana">Champs markoviens conditionnels (<i>Conditional Random Fields</i>) - inférence </font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_crf_inference.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/crf.pdf">pdf</a>] [<a href="diapositives/crf/index.html">html</a>]
		  <ul>
		    <li> Pages 1 à 42 du tutoriel <a href="http://arxiv.org/pdf/1011.4088v1.pdf">An Introduction to Conditional Random Fields</a> de <a href="http://homepages.inf.ed.ac.uk/csutton/">Charles Sutton</a> et <a href="http://people.cs.umass.edu/~mccallum/">Andrew McCallum</a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> Section 8.3 et 8.4 du
		    livre <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/Bishop-PRML-sample.pdf"><i>Pattern
		    Recognition and Machine Learning</i></a> de
		      <a href="http://research.microsoft.com/en-us/um/people/cmbishop/">Christopher M. Bishop</a>
		    <li> Section 8.1 et 8.2 du
		    livre <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/Bishop-PRML-sample.pdf"><i>Pattern
		    Recognition and Machine Learning</i></a> 
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Vidéos du cours <i>Probabilistic Graphical Models</i> de Stanford sur les champs markoviens conditionnels:
		      <ul>
			<li> <a href="http://www.youtube.com/watch?v=5R5ixMmKQzg&feature=relmfu">facteurs</a> (<i>factors</i>)
			<li> champs/réseaux markoviens (<i>Markov networks</i>) [<a href="http://www.youtube.com/watch?v=SH1K4RtX9uQ">1</a] [<a href="http://www.youtube.com/watch?v=kFcjl3A9QuA&feature=relmfu">2</a>] [<a href="http://www.youtube.com/watch?v=giQPlyhlMDU&feature=relmfu">3</a>] 
			<li> <a href="http://www.youtube.com/watch?v=2BXoj778YU8&feature=results_main&playnext=1&list=PL50E6E80E8525B59C">champs markoviens conditionnels</a> (<i>conditional random fields</i>)
			<li> paramétrisation des facteurs [<a href="http://www.youtube.com/watch?v=oLJHOZmAxn0&feature=relmfu">1</a>] [<a href="http://www.youtube.com/watch?v=yRnmTveoFjs&feature=relmfu">2</a>]
			<li> <i>belief propagation</i>/<i>sum product</i> [<a href="http://www.youtube.com/watch?v=ASsKAaHlhCU&feature=relmfu">1</a>] [<a href="http://www.youtube.com/watch?v=gaiZ0N_gPoY&feature=relmfu">2</a>] [<a href="http://www.youtube.com/watch?v=6k7o3-UzUM0&feature=relmfu">3</a>] [<a href="http://www.youtube.com/watch?v=XcUEUZtRLqc&feature=relmfu">4</a>] [<a href="http://www.youtube.com/watch?v=bk8jBNFWZ0I&feature=relmfu">5</a>]
			  <li> <i>max product</i> [<a href="http://www.youtube.com/watch?v=CH1bCDe6k88&feature=relmfu">1</a>] [<a href="http://www.youtube.com/watch?v=OLb6w9h4ll0&feature=relmfu">2</a>]
		      </ul>
		    <li> <a href="http://videolectures.net/cikm08_elkan_llmacrf/">Tutoriel</a> de <a href="http://cseweb.ucsd.edu/~elkan/">Charles Elkan</a>
		  </ul>
		  
		</td> 
		
	      </tr>
	      
	      <tr>
		<td>VE 28/09</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Champs markoviens conditionnels (<i>Conditional Random Fields</i>) - apprentissage</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_crf_apprentissage.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/crf_learn.pdf">pdf</a>] [<a href="diapositives/crf_learn/index.html">html</a>]
		  <ul>
		    <li> Page 43 jusqu'à la fin du tutoriel <a href="http://arxiv.org/pdf/1011.4088v1.pdf">An Introduction to Conditional Random Fields</a> de <a href="http://homepages.inf.ed.ac.uk/csutton/">Charles Sutton</a> et <a href="http://people.cs.umass.edu/~mccallum/">Andrew McCallum</a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> Articles discutant des variantes non-linéaires (avec neurones cachés) des champs markoviens conditionnels:
		      <ul id="contenucours">
			<li> <a href="http://homes.cs.washington.edu/~lfb/paper/nips09b.pdf"><i>Conditional Neural Fields</i></a> de <a href="http://people.csail.mit.edu/jpeng/">Jian Peng</a>, <a href="http://www.cs.washington.edu/homes/lfb/">Liefeng Bo</a> et <a href="http://ttic.uchicago.edu/~jinbo/">Jinbo Xu</a>
			<li> <a href="http://publications.idiap.ch/downloads/papers/2010/Do_AISTATS_2010.pdf"><i>Neural conditional random fields</i></a> de Trinh-Minh-Tri Do et <a href="http://www-connex.lip6.fr/~artieres/Home/pmwiki.php">Thierry Artière</a>
		      </ul>
		    <li> Article précurseur des champs markoviens conditionnels:
		      <ul  id="contenucours">
			<li> <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"><i>Gradient-Based Learning Applied to Document Recognition</i></a> de <a href="http://yann.lecun.com/">Yann LeCun</a>, <a href="http://leon.bottou.org/">Léon Bottou</a>, <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> and <a href="http://www2.research.att.com/~haffner/">Patrick Haffner</a>
		      </ul>
		    <li> Articles sur des modèles alternatifs aux champs markoviens conditionnels:
		      <ul  id="contenucours">
			<li> <b><i>Structured Perceptron:</i></b> <a href="http://acl.ldc.upenn.edu/W/W02/W02-1001.pdf"><i>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</i></a> de <a href="http://www.cs.columbia.edu/~mcollins/">Michael Collins</a>
			<li> <b><i>Structured SVM:</i></b> <a href="http://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2007_3/ratliff_nathan_2007_3.pdf"><i>(Online) Subgradient Methods for Structured Prediction</i></a> de Nathan Ratliff, <a href="http://www.ri.cmu.edu/person.html?person_id=689">Andrew Bagnell</a> et <a href="http://martin.zinkevich.org/">Martin Zinkevich</a> [<a href="http://videolectures.net/cmulls08_ratliff_ssmmt/">video</a>]
		      </ul>
            <li> Article décrivant diverses approches pour tenir compte de la fonction d'erreur selon laquelle le modèle est évalué:
              <ul  id="contenucours">
                <li> <a href="http://arxiv.org/pdf/1107.1805v1.pdf"><i>Loss-sensitive Training of Probabilistic Conditional Random Fields </i></a>  de <a href="http://www.cs.toronto.edu/~mvolkovs/">Maksims Volkovs</a>,  <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a> et <a href="http://www.cs.toronto.edu/~zemel/">Rich Zemel</a>
              </ul>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Vidéo du cours <i>Probabilistic Graphical Models</i> de Stanford sur l'entraînement de champs markoviens conditionnels [<a href="http://www.youtube.com/watch?v=Yr3YmGTXLT4&feature=relmfu">1</a>] [<a href="http://www.youtube.com/watch?v=A4-nPUW81qU">2</a>] [<a href="http://www.youtube.com/watch?v=EqQNyiQ5fFs&feature=relmfu">3</a>]
		    <li> Présentation <a href="http://videolectures.net/iiia06_pereira_slm/"><i>Structured Linear Models</i></a> de <a href="http://www.cis.upenn.edu/~pereira/">Fernando Pereira</a>
		  </ul>

	      <tr bgcolor="#EEE">
		<td>VE 05/10</td> 
		<td>Semaine libre</td> 
	      </tr>
	      
	      <tr>
		<td>VE 12/10</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Machine de Boltzmann restreintes (<i>Restricted Boltzmann Machines</i>)</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_rbm.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/rbm.pdf">pdf</a>] [<a href="diapositives/rbm/index.html">html</a>]
		  <ul>
		    <li> Section 1.4 (excepté la section 1.4.2) du rapport
		      <a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">
			<i>Learning Multiple Layers of Features from Tiny Images</i></a>
		      de <a href="http://www.cs.toronto.edu/~kriz/">Alex Krizhevsky</a>
		    <li>  Sections 5.1 à 5.3 de <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf"><i>Learning Deep Architectures for AI</i></a> de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		    <li> Rapport technique <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"><i>A Practical Guide to Training
			  Restricted Boltzmann Machines</i></a> de <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> Articles évaluant plusieurs approches pour entraîner un modèle avec constante de normalisation:
		      <ul id="contenucours">
			<li> <a href="http://people.cs.umass.edu/~marlin/research/papers/aistats2010-paper.pdf"><i>Inductive Principles for Restricted Boltzmann Machine Learning</i></a> de
			  <a href="http://people.cs.umass.edu/~marlin/">Benjamin
			    Marlin</a>, <a href="http://www.cs.toronto.edu/~kswersky/">Kevin
			    Swersky</a>, <a href="http://www.cs.ubc.ca/~bochen/Dave_Chens_Homepage.html">Bo Chen</a>
			  et <a href="http://www.cs.ubc.ca/~nando/">Nando de Freitas</a>
			<li> <a href="http://jmlr.csail.mit.edu/proceedings/papers/v9/gutmann10a/gutmann10a.pdf"><i>Noise-contrastive estimation: A new estimation principle for
			      unnormalized statistical models</i></a>
			  de <a href="https://sites.google.com/site/michaelgutmann/">Michael Gutmann</a>
			  et <a href="http://www.cs.helsinki.fi/u/ahyvarin/">Aapo Hyvärinen</a>
		      </ul>
		    <li> Articles sur différentes extensions de la machine de Boltzmann restreinte:
		      <ul  id="contenucours">
			<li> <a href="http://www.dmi.usherb.ca/~larocheh/publications/icml-2008-discriminative-rbm.pdf"><i>Classification using Discriminative Restricted Boltzmann Machines</i></a> de <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a> et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> [<a href="http://videolectures.net/icml08_larochelle_cud/">video</a>]
			<li> <a href="http://www.cs.toronto.edu/~rfm/pubs/factored.pdf"><i>Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines</i></a> 
			  de <a href="http://www.cs.toronto.edu/~rfm/">Roland Memisevic</a> et <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>
			<li> <a href="http://www.cs.toronto.edu/~ranzato/publications/ranzato_aistats2010.pdf"><i>Factored 3-Way Restricted Boltzmann Machines for Modeling Natural Images</i></a> 
			  de <a href="http://www.cs.toronto.edu/~ranzato/">Marc'Aurelio Ranzato</a>, 
			  <a href="http://www.cs.toronto.edu/~kriz/">Alex Krizhevsky</a> et 
			  <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> [<a href="http://videolectures.net/aistats2010_ranzato_f3wr/">video</a>]
			<li> <a href="http://web.eecs.umich.edu/~honglak/nips07-sparseDBN.pdf"><i>Sparse deep belief net model for visual area V2</i></a> 
			  de <a href="http://web.eecs.umich.edu/~honglak/">Honglak Lee</a>, <a href="http://math.nyu.edu/~chaitu/">Chaitanya Ekanadham</a> 
			  et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
			<li> <a href="http://www.cs.toronto.edu/~hinton/absps/reluICML.pdf"><i>Rectified Linear Units Improve Restricted Boltzmann Machines</i></a> de
			  <a href="http://www.cs.toronto.edu/~vnair/">Vinod Nair</a> et <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> 
			<li> <a href="http://www.csri.utoronto.ca/~hinton/absps/nips00-ywt.pdf"><i>Rate-coded Restricted Boltzmann Machines for Face Recognition</i></a> de
			  <a href="http://www.gatsby.ucl.ac.uk/~ywteh/">Yee Whye Teh</a> et <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> 
			<li> <a href="http://www.icml-2011.org/papers/591_icmlpaper.pdf"><i>
			      Unsupervised Models of Images by Spike-and-Slab RBMs</i></a>
			  de <a href="http://aaroncourville.wordpress.com/">Aaron Courville</a>,
			  <a href="http://people.fas.harvard.edu/~bergstra/">James Bergstra</a>
			  et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> [<a href="http://techtalks.tv/talks/unsupervised-models-of-images-by-spike-and-slab-rbms/54326/">video</a>]
		      </ul>
		    <li> Articles sur des techniques avancées d'échantillonnage:
		      <ul id="contenucours">
			<li> Section 30.1 du livre <a href="http://www.cs.toronto.edu/~mackay/itprnn/book.pdf"><i>Information Theory, Inference, and Learning Algorithms</i></a> de <a href="http://www.inference.phy.cam.ac.uk/mackay/">David MacKay</a>
			<li> <a href="http://www.utstat.toronto.edu/~rsalakhu/papers/trans.pdf"><i>Learning in Markov Random Fields using
			      Tempered Transitions</i></a> de <a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a>
			<li> <a href="http://jmlr.csail.mit.edu/proceedings/papers/v9/desjardins10a/desjardins10a.pdf"><i>Parallel Tempering for Training of Restricted Boltzmann Machines</i></a> de
			  <a href="http://brainlogging.wordpress.com/">Guillaume Desjardins</a>, <a href="http://aaroncourville.wordpress.com/">Aaron Courville</a>, 
			  <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>, <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a>
			  et <a href="http://www.iro.umontreal.ca/~delallea/">Olivier Delalleau</a>
			<li> <a href="http://www.utstat.toronto.edu/~rsalakhu/papers/adapt.pdf"><i>Learning Deep Boltzmann Machines using Adaptive MCMC</i></a> 
			  de <a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a>
		      </ul>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Présentation <a href="http://videolectures.net/nips09_hinton_dlmi/"><i>Deep Learning with Multiplicative Interactions</i></a> 
		      de <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> sur plusieurs variantes de machine
		      de Boltzmann restreintes
		  </ul>

		</td>
	      </tr>
	      
	      <tr bgcolor="#EEE">
		<td>VE 19/10</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Autoencodeurs</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_autoencoder.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/autoencoder.pdf">pdf</a>] [<a href="diapositives/autoencoder/index.html">html</a>]
		  <ul >
		    <li> Section 5 et <i>Appendix C</i> de 
		      <a href="http://www.dmi.usherb.ca/~larocheh/publications/jmlr-larochelle09a.pdf"><i>Exploring Strategies for Training Deep Neural Networks</i></a>
		      de <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a>, 
		      <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>, Jérôme Louradour et Pascal Lamblin
		    <li> Sections 7 à 7.2.3 de <a href="http://arxiv.org/pdf/1206.5538v2.pdf"><i>Representation Learning: A Review and New Perspectives</i></a> de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>,
		      <a href="http://aaroncourville.wordpress.com/">Aaron Courville</a> et <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a>
		    <li> Article <a href="http://www.dmi.usherb.ca/~larocheh/publications/icml-2008-denoising-autoencoders.pdf"><i>Extracting and Composing Robust Features with Denoising
			  Autoencoders</i></a> de <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a>, <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a>, 
		      <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> et Pierre-Antoine Manzagol [<a href="http://videolectures.net/icml08_vincent_ecrf/">video</a>]
		    <li> Article <a href="http://www.iro.umontreal.ca/~vincentp/Publications/contractive_autoencoder_icml2011.pdf"><i>Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</i></a> 
		      de <a href="http://www-etud.iro.umontreal.ca/~rifaisal/">Salah Rifai</a>, 
		      <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a>, 
		      Xavier Muller, Xavier Glorot et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> Article théorique démontrant qu'une couche non-linéaire n'est pas nécessaire pour obtenir la meilleure erreur de reconstruction:
		      <ul id="contenucours">
			<li> <a href="http://publications.idiap.ch/downloads/reports/2000/rr00-16.pdf">Auto-Association by Multilayer Perceptrons and Singular Value Decomposition</a> de <a href="http://people.idiap.ch/bourlard">Hervé Bourlard</a> et Yves Kamp
		      </ul>
		    <li> Articles sur différentes extensions d'autoencodeurs:
		      <ul id="contenucours">
			<li> <a href="http://www.dmi.usherb.ca/~larocheh/publications/aistats_2009_robust_interdependent.pdf"><i>Deep Learning using Robust Interdependent Codes</i></a> de <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a>, <a href="http://www.dumitru.ca/">Dumitru Erhan</a> et <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a>
			<li> <a href="http://www.cs.toronto.edu/~ranzato/publications/ranzato-icml08.pdf"><i>Semi-supervised Learning of Compact Document Representations with Deep Networks</i></a> 
			  de <a href="http://www.cs.toronto.edu/~ranzato/">Marc'Aurelio Ranzato</a> 
			  et <a href="http://research.microsoft.com/en-us/um/people/szummer/">Martin Szummer</a> [<a href="http://videolectures.net/icml08_szummer_sslcdr/">video</a>]
			<li> <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_embeddings.pdf"><i>Large-Scale Learning of Embeddings with Reconstruction Sampling</i></a> 
			  de <a href="http://ynd.github.com/">Yann Dauphin</a>, Xavier Glorot et 
			  <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> [<a href="http://techtalks.tv/talks/54424/">video</a>]
			<li> <a href="http://www.dmi.usherb.ca/~larocheh/publications/aistats_2012.pdf"><i>On Nonparametric Guidance for Learning Autoencoder Representations</i></a> de
			  <a href="http://www.cs.toronto.edu/~jasper/">Jasper Snoek</a>, <a href="http://people.seas.harvard.edu/~rpa/">Ryan Adams</a> et <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a>
			<li> <a href="http://www.cs.toronto.edu/~rfm/pubs/rae.pdf">Gradient-based learning of higher-order image features</a> 
			  de <a href="http://www.cs.toronto.edu/~rfm/">Roland Memisevic</a>
		      </ul>
		  </ul>
		  </td>
	      </tr>
	      
	      <tr>
		<td>VE 26/10</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Réseaux profonds (<i>Deep learning</i>)</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_reseau_profond.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/deep.pdf">pdf</a>] [<a href="diapositives/deep/index.html">html</a>]
		  <ul>
		    <li> Article <a href="http://www.dmi.usherb.ca/~larocheh/publications/jmlr-larochelle09a.pdf"><i>Exploring Strategies for Training Deep Neural Networks</i></a>
		      de <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a>, 
		      <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>, Jérôme Louradour et Pascal Lamblin
		    <li> Article <a href="http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf"><i>A Fast Learning Algorithm for Deep Belief Nets</i></a> de <a href="http://www.cs.toronto.edu/~hinton">Geoffrey Hinton</a>, Simon Osindero et <a href="http://www.gatsby.ucl.ac.uk/~ywteh/">Yee Whye Teh</a>
		    <li> Article <a href="http://www.cs.toronto.edu/~hinton/science.pdf"><i>Reducing the dimensionality of data with neural networks</i></a> 
		      de <a href="http://www.cs.toronto.edu/~hinton">Geoffrey Hinton</a> 
		      et <a href="http://www.utstat.toronto.edu/~rsalakhu/">Ruslan Salakhutdinov</a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> Article détaillé sur les réseaux profonds:
		      <ul id="contenucours">
			<li> <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">Learning Deep Architectures for AI</a> 
			  de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		      </ul>
		    <li> Évaluations expérimentales de l'apprentissage de réseaux profonds:
		      <ul id="contenucours">
			<li> <a href="http://www.dmi.usherb.ca/~larocheh/publications/deep-nets-icml-07.pdf"><i>An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation</i></a>
			  de <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a>, 
			  <a href="http://www.dumitru.ca/">Dumitru Erhan</a>, 
			  <a href="http://aaroncourville.wordpress.com/">Aaron Courville</a>,
			  <a href="http://people.fas.harvard.edu/~bergstra/">James Bergstra</a>
			  et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
			<li> <a href="http://jmlr.csail.mit.edu/papers/volume11/erhan10a/erhan10a.pdf"><i>Why Does Unsupervised Pre-training Help Deep Learning?</i></a> 
			  de <a href="http://www.dumitru.ca/">Dumitru Erhan</a>, 
			  <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>,
			  <a href="http://aaroncourville.wordpress.com/">Aaron Courville</a>,
			  Pierre-Antoine Manzagol, <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a> et <a href="http://bengio.abracadoudou.com/">Samy Bengio</a>
		      </ul>
		    <li> Diverses alternatives pour l'apprentissage non-supervisé vorace 
		      des couches cachées d'une réseau profond:
		      <ul id="contenucours">
			<li> <a href="http://www.thespermwhale.com/jaseweston/papers/deep_embed.pdf"><i>Deep Learning via Semi-Supervised Embedding</i></a> 
			  de <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a>, 
			  Frédéric Ratle et <a href="http://ronan.collobert.com/">Ronan Collobert</a> 
			  [<a href="http://videolectures.net/icml09_weston_dlss/">video</a>]
			<li> <a href="http://www.thespermwhale.com/jaseweston/papers/embedvideo.pdf"><i>Deep Learning from Temporal Coherence in Video</i></a> de 
			  <a href="http://www.cs.illinois.edu/homes/hmobahi2/">Hossein Mobahi</a>,
			  <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a> et
			  <a href="http://ronan.collobert.com/">Ronan Collobert</a>
			<li> <a href="http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf"><i>Kernel Methods for Deep Learning</i></a> 
			  de <a href="http://cseweb.ucsd.edu/~yoc002/">Youngmin Cho</a> 
			  et <a href="http://cseweb.ucsd.edu/~saul/">Lawrence Saul</a>
			<li> <a href="http://books.nips.cc/papers/files/nips22/NIPS2009_0933.pdf"><i>
			      Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</i></a>
			  de <a href="http://people.fas.harvard.edu/~bergstra">James Bergstra</a>
			  et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		      </ul>
		    <li> D'autres types de réseaux de neurones profonds, qui ne sont pas <i>feed-forward</i>:
		      <ul id="contenucours">
			<li> <a href="http://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf"><i>Deep Boltzmann Machines</i></a>
			  de <a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a> et
			  <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> [<a href="http://videolectures.net/nipsworkshops09_salakhutdinov_ldbm/">video</a>]
			<li> <a href="http://cs.stanford.edu/~jngiam/papers/NgiamChenKohNg2011.pdf"><i>Learning Deep Energy Models</i></a>
			  de <a href="http://cs.stanford.edu/~jngiam/">Jiquan Ngiam</a>, 
			  <a href="http://cs.stanford.edu/~zhenghao/">Zhenghao Chen</a>, 
			  <a href="http://cs.stanford.edu/~pangwei/">Pang Wei Koh</a> 
			  et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> [<a href="http://techtalks.tv/talks/learning-deep-energy-models/54325/">video</a>]
		      </ul>
		      
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Présentation <a href="http://videolectures.net/okt09_bengio_ldhr/"><i>Learning Deep Hierarchies of Representations</i></a> 
		      de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>, donnée à Google
		    <li> Tutoriel <a href="http://videolectures.net/mlss09uk_hinton_dbn/"><i>Deep Belief Nets</i></a> 
		      de <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> 
		      
		  </ul>
		  </td>
	      </tr>
	      
	      <tr  bgcolor="#EEE">
		<td>VE 02/11</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Codage parcimonieux (<i>Sparse coding</i>)</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_codage_parcimonieux.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/sparse_coding.pdf">pdf</a>] [<a href="diapositives/sparse_coding/index.html">html</a>]
		  <ul>
		    <li> Sections 1 à 4 de l'article 
		      <a href="http://web.eecs.umich.edu/~honglak/nips06-sparsecoding.pdf">
			<i>Efficient sparse coding algorithms</i></a> 
		      de <a href="http://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
		      <a href="http://www.stanford.edu/~ajbattle/">Alexis Battle</a>,
		      <a href="http://ai.stanford.edu/~rajatr/">Rajat Raina</a> et
		      <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
		      <ul>
			<li> Pour en savoir plus sur le Lagrangien: sections 5.1.1 à 5.1.5 du livre 
			  <a href="http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"><i>Convex Optimization</i></a>
			  de <a href="http://www.stanford.edu/~boyd/">Stephen Boyd</a> et 
			  <a href="http://www.ee.ucla.edu/~vandenbe/">Lieven Vandenberghe</a>
		      </ul>
		    <li> Sections 1 et 2 de l'article 
		      <a href="http://www.cs.nyu.edu/~kgregor/gregor-icml-10.pdf">
			<i>Learning Fast Approximations of Sparse Coding</i></a> 
		      de <a href="http://www.cs.nyu.edu/~kgregor">Karol Gregor</a> et <a href="http://yann.lecun.com/">Yann LeCun</a>
		    <li> Article 
		      <a href="http://www.cs.stanford.edu/people/ang//papers/icml07-selftaughtlearning.pdf"><i>Self-taught Learning: Transfer Learning from Unlabeled Data</i></a>
		      de <a href="http://ai.stanford.edu/~rajatr/">Rajat Raina</a>,
		      <a href="http://www.stanford.edu/~ajbattle/">Alexis Battle</a>,
		      <a href="http://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
		      <a href="http://www.stanford.edu/~bpacker/">Benjamin Packer</a> et
		      <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
		    <li> Article
		      <a href="http://people.cs.umass.edu/~eshelham/files/papers/olshausen-field-emergence-simple-cells.pdf">
			<i>Emergence of simple cell receptive field properties by learning a sparse code for natural images</i></a>
		      de <a href="http://redwood.berkeley.edu/bruno/">Bruno Olshausen</a> et 
		      <a href="http://redwood.psych.cornell.edu/people/david.html">David Field</a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> D'autres algorithmes apprenant une représentation parcinomieuse:
		      <ul id="contenucours">
			<li> <a href="http://arxiv.org/pdf/1010.3467.pdf"><i>Fast Inference in Sparse Coding Algorithms
			      with Applications to Object Recognition</i></a> 
			  de <a href="http://koray.kavukcuoglu.org/">Koray Kavukcuoglu</a>,
			  <a href="http://www.cs.toronto.edu/~ranzato/">Marc'Aurelio Ranzato</a> et
			  <a href="http://yann.lecun.com/">Yann LeCun</a>
			<li> <i>Independent Component Analysis (ICA)</i>: <a href="http://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf"><i>
			      Independent Component Analysis: Algorithms and Applications</i></a>
			  de  <a href="http://www.cs.helsinki.fi/u/ahyvarin/">Aapo Hyvärinen</a> et
			  <a href="http://users.ics.aalto.fi/oja/">Erkki Oja</a>
			<li> <i>Reconstruction ICA</i>: 
			  <a href="http://ai.stanford.edu/~quocle/LeKarpenkoNgiamNg.pdf">
			    <i>ICA with Reconstruction Cost for Efficient
			      Overcomplete Feature Learning</i></a> de 
			  <a href="http://ai.stanford.edu/~quocle/">Quoc Le</a>,
			  Alexandre Karpenko,
			  <a href="http://cs.stanford.edu/~jngiam/">Jiquan Ngiam</a>
			  et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
			<li> <a href="http://research2.fit.edu/ice/sites/default/files/aharon_elad_bruckstein_2006_0.pdf"><i>K-SVD: An Algorithm for Designing Overcomplete
			      Dictionaries for Sparse Representation</i></a> de
			  <a href="http://www.cs.technion.ac.il/~michalo/">Michal Aharon</a>
			  <a href="http://www.cs.technion.ac.il/~elad/">Michael Elad</a>
			  et <a href="http://www.cs.technion.ac.il/~freddy/">Alfred Bruckstein</a>
			<li> <a href="http://arxiv.org/pdf/1206.6407.pdf">
			    <i>Large-Scale Feature Learning With Spike-and-Slab Sparse Coding</i></a>
			  de <a href="http://www-etud.iro.umontreal.ca/~goodfeli/">Ian Goodfellow</a>,
			  <a href="http://aaroncourville.wordpress.com/">Aaron Courville</a>
			  et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> 
		      </ul>
		    <li> Variantes de codage parcimonieux:
		      <ul id="contenucours">
			<li> <a href="http://ai.stanford.edu/~rajatr/papers/expsc_ijcai09.pdf"><i>Exponential Family Sparse Coding with Applications to Self-taught Learning</i></a>
			  de <a href="http://web.eecs.umich.edu/~honglak/">Honglak Lee</a>, 
			  <a href="http://ai.stanford.edu/~rajatr/">Rajat Raina</a>, 
			  <a href="http://cs.stanford.edu/people/teichman/">Alex Teichman</a> et
			  <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
			<li> <a href="http://www.di.ens.fr/~jenatton/paper/HierarchicalDictionaryLearningICML2010.pdf"><i>Proximal Methods for Sparse Hierarchical Dictionary Learning</i></a>
			  de <a href="http://www.di.ens.fr/~jenatton/">Rodolphe Jenatton</a>,
			  <a href="http://www.di.ens.fr/~mairal/">Julien Mairal</a>,
			  <a href="http://www.di.ens.fr/~obozinski/">Guillaume Obozinski</a> et
			  <a href="http://www.di.ens.fr/~fbach/">Francis Bach</a>
		      </ul>
		    <li> Apprentissage en ligne de codage parcimonieux:
		      <ul id="contenucours">
			<li> <a href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf"><i>Online Dictionary Learning for Sparse Coding</i></a> de 
			  <a href="http://www.di.ens.fr/~mairal/">Julien Mairal</a>,
			  <a href="http://www.di.ens.fr/~fbach/">Francis Bach</a>,
			  <a href="http://www.di.ens.fr/~ponce/">Jean Ponce</a> et
			  <a href="http://www.ece.umn.edu/~guille/">Guillermo Sapiro</a> [<a href="http://videolectures.net/icml09_mairal_odlsc/">video</a>]
		      </ul>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Tutoriel <a href="http://videolectures.net/nips09_bach_smm/">
			<i>Sparse Methods for Machine Learning: Theory and Algorithms Francis Bach</i></a>
		      de <a href="http://www.di.ens.fr/~fbach/">Francis Bach</a><br>
		      (couvre l'estimation parcimonieuse en général, pas seulement le codage parcimonieux)
		  </ul>
		  </td>
		</td> 
	      </tr>
	      
	      <tr>
		<td>VE 09/11</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Vision par ordinateur avec réseaux de neurones</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_reseau_convolution.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/convolutional_network.pdf">pdf</a>] [<a href="diapositives/convolutional_network/index.html">html</a>] 
		  <ul>
		    <li> Article <a href="http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf">
			<i>What is the Best Multi-Stage Architecture for Object Recognition?</i></a>
		      de Kevin Jarrett, 
		      <a href="http://koray.kavukcuoglu.org/">Koray Kavukcuoglu</a>,
		      <a href="http://www.cs.toronto.edu/~ranzato/">Marc'Aurelio Ranzato</a> et
		      <a href="http://yann.lecun.com/">Yann LeCun</a>
		    <li> Article <a href="http://research.microsoft.com/en-us/um/people/jplatt/ICDAR03.pdf"><i>
			  Best Practices for Convolutional Neural Networks 
			  Applied to Visual Document Analysis</i></a>
		      de Patrice Simard, 
		      Dave Steinkraus et
		      <a href="http://research.microsoft.com/en-us/people/jplatt/">John Platt</a>
		    <li> Article <a href="http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf">
			<i>Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations</i></a>
		      <a href="http://web.eecs.umich.edu/~honglak/">Honglak Lee</a>, 
		      <a href="http://people.csail.mit.edu/rgrosse/">Roger Grosse</a>,
		      <a href="http://www.cs.princeton.edu/~rajeshr/">Rajesh Ranganath</a>
		      et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> [<a href="http://videolectures.net/icml09_lee_cdb/">video</a>]
		    <li> Section 1.1 à 1.3.3 et <i>Appendix A</i> du rapport
		      <a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">
			<i>Learning Multiple Layers of Features from Tiny Images</i></a>
		      de <a href="http://www.cs.toronto.edu/~kriz/">Alex Krizhevsky</a>
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> Résumé de la neurophysionomie du cortex visuel:
		      <ul id="contenucours">
			<li> Section 2.3 de la 
			  <a href="http://people.fas.harvard.edu/~bergstra/files/pub/11_These.pdf">thèse</a> 
			  de <a href="http://people.fas.harvard.edu/~bergstra/">James Bergstra</a>
		      </ul>
		    <li> Analyse de l'utilisation de filtres aléatoires:
		      <ul id="contenucours">
			<li> <a href="http://www.stanford.edu/~asaxe/papers/Saxe%20et%20al.%20-%202011%20-%20On%20Random%20Weights%20and%20Unsupervised%20Feature%20Learning.pdf">
			    <i>On random weights and unsupervised feature learning
			      </i></a>
			  de <a href="http://www.stanford.edu/~asaxe/">Andrew Saxe</a>, 
			  <a href="http://cs.stanford.edu/~pangwei/">Pang Wei Koh</a>,
			  <a href="http://cs.stanford.edu/~zhenghao/">Zhenghao Chen</a>,
			  Maneesh Bhand, Bipin Suresh 
			  et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> [<a href="http://techtalks.tv/talks/54303/">video</a>]
		      </ul>
		    <li> Différentes applications de vision par ordinateur:
		      <ul id="contenucours">
			<li> <a href="http://yann.lecun.com/exdb/publis/pdf/farabet-icml-12.pdf">
			    <i>Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers</i></a>
			  de <a href="http://www.clement.farabet.net/">Clément Farabet</a>,
			  <a href="http://cs.nyu.edu/~ccouprie/">Camille Couprie</a>,
			  <a href="http://www.najman.org/">Laurent Najman</a> et
			  <a href="http://yann.lecun.com/">Yann LeCun</a>
			<li> <a href="http://www.idsia.ch/~juergen/nips2009.pdf"><i>
			      Offine Handwriting Recognition with
			      Multidimensional Recurrent Neural Networks</i></a>
			  de <a href="http://www6.in.tum.de/Main/Graves">Alex Graves</a>
			  et <a href="http://www.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
		      </ul>
		    <li> D'autres approches convolutionnelles:
		      <ul id="contenucours">
			<li> <a href="http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf">
			    <i>An Analysis of Single-Layer Networks in Unsupervised Feature Learning</i></a>
			  de <a href="http://www.stanford.edu/~acoates/">Adam Coates</a>,
			  <a href="http://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
			  <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>
			<li> <a href="http://www.stanford.edu/~acoates/papers/coatesng_icml_2011.pdf"><i>
			      The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization</i></a>
			  de <a href="http://www.stanford.edu/~acoates/">Adam Coates</a>
			  <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> [<a href="http://techtalks.tv/talks/the-importance-of-encoding-versus-training-with-sparse-coding-and-vector-quantization/54301/">video</a>]
			<li> <a href="http://web.eecs.umich.edu/~honglak/icml12-invariantFeatureLearning.pdf">
			    <i>Learning Invariant Representations with Local Transformations</i></a>
			  de Kihyuk Sohn et <a href="http://web.eecs.umich.edu/~honglak/">Honglak Lee</a>
			  [<a href="http://techtalks.tv/talks/57420/">video</a>]
			<li> <a href="http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf">
			    <i>Adaptive Deconvolutional Networks for Mid and High Level Feature Learning</i></a>
			  de <a href="http://www.matthewzeiler.com/">Matthew Zeiler</a>,
			  <a href="http://www.uoguelph.ca/~gwtaylor/">Graham Taylor</a> et
			  <a href="http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php">Rob Fergus</a>
		      </ul>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Présentation <a href="http://videolectures.net/mlss09us_lecun_lfh/">
			<i>Learning Feature Hierarchies</i></a> de <a href="http://yann.lecun.com/">Yann LeCun</a>
		  </ul>
		  </td>
		</td> 
	      </tr>
	      
	      <tr bgcolor="#EEE">
		<td>VE 16/11</td> 
		<td><b><font face="Copperplate,georgia,Verdana">Traitement automatique de la langue avec réseaux de neurones</font></b><br>
		  <br><b>- Lectures obligatoires -</b> [<a href="questions_taln.html">questions de lecture</a>] | Diapositives: [<a href="https://larocheh.github.io/ift725/nlp-word-representations.pdf">pdf-1</a>] [<a href="https://larocheh.github.io/ift725/nlp-language-model.pdf">pdf-2</a>] [<a href="https://larocheh.github.io/ift725/nlp-tagging.pdf">pdf-3</a>] [<a href="https://larocheh.github.io/ift725/nlp-recursive-net.pdf">pdf-4</a>] [<a href="diapositives/nlp-word-representations/index.html">html-1</a>] [<a href="diapositives/nlp-language-model/index.html">html-2</a>] [<a href="diapositives/nlp-tagging/index.html">html-3</a>] [<a href="diapositives/nlp-recursive-net/index.html">html-4</a>]
		  <ul>
		    <li> Article Scholarpedia <a href="http://www.scholarpedia.org/article/Neural_net_language_models">
			<i>Neural net language models</i></a>
		      de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> 
		    <li> Article <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf">
			<i>Hierarchical Probabilistic Neural Network Language Model</i></a>
		      de Frédéric Morin et 
		      <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> 
		    <li> Article <a href="http://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf">
			<i>A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</i></a>
		      de <a href="http://ronan.collobert.com/">Ronan Collobert</a> 
		      et <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a>
		    <li> Article <a href="http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf">
			<i>Parsing Natural Scenes and Natural Language with Recursive Neural Networks</i></a>
		      de <a href="http://www.socher.org/">Richard Socher</a>, 
		      Jeffrey Pennington, 
		      <a href="http://ai.stanford.edu/~ehhuang/">Eric Huang</a>, 
		      <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> et
		      <a href="http://nlp.stanford.edu/~manning/">Christopher Manning</a>
		      [<a href="http://techtalks.tv/talks/54422/">video</a>]
		  </ul><br>
		  <b>- Lectures suggérées -</b>
		  <ul>
		    <li> D'autres articles sur la modélisation du language avec réseaux de neurones:
		      <ul id="contenucours">
			<li> <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">
			    <i>Recurrent neural network based language model</i></a>
			  de <a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/">Tomas Mikolov</a>,
			  <a href="http://www.fit.vutbr.cz/~karafiat/">Martin Karafiat</a>,
			  <a href="http://www.fit.vutbr.cz/~burget/">Lukas Burget</a>,
			  <a href="http://www.fit.vutbr.cz/~cernocky/"Jan Cernocky</a> et
			  <a href="http://old-site.clsp.jhu.edu/~sanjeev/">Sanjeev Khudanpur</a>
			<li> <a href="http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf">
			    <i>Generating Text with Recurrent Neural Network</i></a>
			  de <a href="http://www.cs.utoronto.ca/~ilya">Ilya Sutskever</a>,
			  <a href="http://www.cs.toronto.edu/~jmartens/">James Martens</a> et
			  <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> [<a href="http://techtalks.tv/talks/54425/">video</a>] [<a href="http://www.cs.utoronto.ca/~ilya/rnn.html">demo</a>]
			<li> <a href="ftp://tlp.limsi.fr/public/emnlp05.pdf"><i> 
			      Training Neural Network Language Models
			      On Very Large Corpora</i></a>
			  de <a href="http://www-lium.univ-lemans.fr/~schwenk/">Holger Schwenk</a>
			  et <a href="http://www.limsi.fr/~gauvain/">Jean-Luc Gauvain</a>
			<li> <a href="http://aclweb.org/anthology-new/N/N12/N12-1005.pdf">
			    <i>Continuous Space Translation Models with Neural Networks</i></a>
			  de <a href="http://perso.limsi.fr/Individu/lehaison/wiki/doku.php">Le Hai Son</a>,
			  <a href="http://perso.limsi.fr/allauzen/wiki/index.php/Accueil">Alexandre Allauzen</a>
			  et <a href="http://perso.limsi.fr/Individu/yvon/mysite/mysite.php">François Yvon</a>
		      </ul>
		    <li> Modélisation de documents avec réseaux de neurones:
		      <ul id="contenucours">
			<li> <a href="http://www.utstat.toronto.edu/~rsalakhu/papers/semantic_final.pdf">
			    <i>Semantic hashing</i></a>
			  de <a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a>
			  et <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>
			<li> <a href="http://books.nips.cc/papers/files/nips22/NIPS2009_0817.pdf">
			    <i>Replicated Softmax: an Undirected Topic Model</i></a>
			  de <a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a>
			  et <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>			  
		      </ul>
		    <li> D'autres articles sur l'étiquetage de mots basée sur un réseau de neurones:
		      <ul id="contenucours">
			<li> <a href="http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf">
			    <i>Natural Language Processing (Almost) from Scratch</i></a>
			  de <a href="http://ronan.collobert.com/">Ronan Collobert</a>,
			  <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a>,
			  <a href="http://leon.bottou.org/">Léon Bottou</a>,
			  Michael Karlen,
			  <a href="http://koray.kavukcuoglu.org/">Koray Kavukcuoglu</a>,
			  <a href="http://paul.rutgers.edu/~pkuksa/">Pavel Kuksa</a>
			  [<a href="http://research.microsoft.com/apps/video/default.aspx?id=115867">video</a>]
			<li> <a href="http://ronan.collobert.com/pub/matos/2011_parsing_aistats.pdf">
			    <i>Deep Learning for Efficient Discriminative Parsing</i></a>
			  de <a href="http://ronan.collobert.com/">Ronan Collobert</a> 
			  [<a href="http://videolectures.net/aistats2011_collobert_deep/">video</a>]
		      </ul>
		    <li> D'autres algorithmes d'apprentissage efficaces pour données textuelles:
		      <ul id="contenucours">
			<li> <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf">
			    <i>Quick Training of Probabilistic Neural Nets by Importance Sampling</i></a>
			  de <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
			  et Jean-Sébastien Senécal
			<li> <a href="http://www.gatsby.ucl.ac.uk/~amnih/papers/hlbl_final.pdf"><i>
			      A Scalable Hierarchical Distributed Language Model</i></a>
			  de <a href="http://www.gatsby.ucl.ac.uk/~amnih">Andriy Mnih</a>
			  et <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>
			<li> <a href="http://www.gatsby.ucl.ac.uk/~amnih/papers/ncelm.pdf">
			    <i>A fast and simple algorithm for training neural probabilistic language models</i></a>
			  de <a href="http://www.gatsby.ucl.ac.uk/~amnih">Andriy Mnih</a>
			  et <a href="http://www.gatsby.ucl.ac.uk/~ywteh/">Yee Whye Teh</a>
			<li> <a href="http://www.dmi.usherb.ca/~larocheh/publications/wrrbm_icml2012.pdf">
			    <i>Training Restricted Boltzmann Machines on Word Observations</i></a>
			  de <a href="http://www.cs.toronto.edu/~gdahl/">George Dahl</a>
			  <a href="http://people.seas.harvard.edu/~rpa/">Ryan Adams</a> 
			  et <a href="http://www.dmi.usherb.ca/~larocheh/">Hugo Larochelle</a>
			<li> <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_embeddings.pdf">
			    <i>Large-Scale Learning of Embeddings with Reconstruction Sampling</i></a> 
			  de <a href="http://ynd.github.com/">Yann Dauphin</a>, Xavier Glorot et 
			  <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> [<a href="http://techtalks.tv/talks/54424/">video</a>]
		      </ul>
		    <li> Apprentissage de représentation des mots:
		      <ul id="contenucours">
			<li> <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf">
			    <i>Word representations: A simple and general method for semi-supervised learning</i></a>
			  de <a href="http://www-etud.iro.umontreal.ca/~turian/">Joseph Turian</a>,
			  <a href="http://lev-arye.com/">Lev Ratinov</a>
			  et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
			<li> <a href="http://www.pdhillon.com/nips11dhillon.pdf">
			    <i>Multi-View Learning of Word Embeddings via CCA</i></a>
			  de <a href="http://www.pdhillon.com/">Paramveer Dhillon</a>,
			  <a href="http://gosset.wharton.upenn.edu/~foster/index.pl">Dean Foster</a>
			  et <a href="http://www.cis.upenn.edu/~ungar/">Lyle Ungar</a>
			<li> <a href="https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf">
			    <i>Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing</i></a>
			  de <a href="https://www.hds.utc.fr/~bordesan/dokuwiki/doku.php">Antoine Bordes</a>,
			  Xavier Glorot,
			  <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a>
			  et <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
			<li> <a href="http://www.socher.org/uploads/Main/HuangSocherManning_ACL2012.pdf"><i>
			      Improving Word Representations via Global Context
			      and Multiple Word Prototypes</i></a>
			  de <a href="http://ai.stanford.edu/~ehhuang/">Eric Huang</a>, 
			  <a href="http://www.socher.org/">Richard Socher</a>, 
			  <a href="http://nlp.stanford.edu/~manning/">Christopher Manning</a>
			  et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>			  
		      </ul>
		    <li> Variantes du <i>recursive neural network</i>:
		      <ul id="contenucours">
			<li> <a href="http://www.socher.org/uploads/Main/SocherPenningtonHuangNgManning_EMNLP2011.pdf">
			    <i>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</i></a>
			  de <a href="http://www.socher.org/">Richard Socher</a>, 
			  Jeffrey Pennington, 
			  <a href="http://ai.stanford.edu/~ehhuang/">Eric Huang</a>, 
			  <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> et
			  <a href="http://nlp.stanford.edu/~manning/">Christopher Manning</a>
			<li> <a href="http://www.socher.org/uploads/Main/SocherHuangPenningtonNgManning_NIPS2011.pdf">
			    <i>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</i></a>
			  de <a href="http://www.socher.org/">Richard Socher</a>, 
			  <a href="http://ai.stanford.edu/~ehhuang/">Eric Huang</a>, 
			  Jeffrey Pennington, 
			  <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> et
			  <a href="http://nlp.stanford.edu/~manning/">Christopher Manning</a>
			<li> <a href="http://www.socher.org/uploads/Main/SocherHuvalManningNg_EMNLP2012.pdf">
			    <i>Semantic Compositionality through Recursive Matrix-Vector Spaces</i></a>
			  de <a href="http://www.socher.org/">Richard Socher</a>, 
			  Brody Huval,
			  <a href="http://nlp.stanford.edu/~manning/">Christopher Manning</a>
			  et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a>			  
		      </ul>
		  </ul><br>
		  <b>- Matériel multimédia suggéré -</b>
		  <ul>
		    <li> Tutoriel <a href="http://videolectures.net/nips09_collobert_weston_dlnl/">
			<i>Deep Learning in Natural Language Processing</i></a> 
		      de <a href="http://ronan.collobert.com/">Ronan Collobert</a>
		      et <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a>
		    <li> Tutoriel <a href="http://www.youtube.com/watch?v=IF5tGEgRCTQ&list=PL4617D0E28A5781B0">
			<i>Deep Learning for NLP (without Magic)</i></a> de
		      <a href="http://www.socher.org/">Richard Socher</a>, 
		      <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a> et
		      <a href="http://nlp.stanford.edu/~manning/">Christopher Manning</a>
		  </ul>
		  </td>
		</td> 
	      </tr>
	      
	      <tr>
		<td>VE 23/11</td> 
		<td><b>Présentations orales</b></td> 
	      </tr>
	      
	      <tr bgcolor="#EEE">
		<td>VE 30/11</td> 
		<td><b>Présentations orales</b></td> 
	      </tr>
	      
	    </table>
	  </div>
	  
	  <div id="footer">
	  </div>
	  
	</div>
      </div>


    </body>
  </html>
