<!DOCTYPE html >
<html >
  <head>
    <title>Hugo Larochelle</title>
    <meta http-equiv="content-type" content="text/html; charset=iso-8859-1" />
    <link rel="stylesheet"
          href="http://fonts.googleapis.com/css?family=Droid+Sans:regular,bold"
          type="text/css" />
    <link rel="stylesheet" href="../../css/2.css" type="text/css" media="screen,projection" />

  </head>
  
  <body>

    <div id="wrapper">
      <div id="innerwrapper">

	<div id="header">
      <div id="header-text">
	    Hugo Larochelle
	  <!-- <h1 id="header-right"></h1>-->
	    <ul id="nav">
	      
	      <li><a href="../../index_fr.html" accesskey="a"><em>A</em>ccueil</a></li>
	      
	      <li><a href="../../publications_fr.html" accesskey="p"><em>P</em>ublications</a></li>

	      <li><a href="../../university_fr.html" accesskey="u" class="active"><em>U</em>niversité</a></li>

	      <li><a href="../../links_fr.html" accesskey="l"><em>L</em>iens</a></li>

	    </ul>
	  </div>
	  </div>
	  <div id="sidebar">
	    <h2>IFT 725 <small>(Automne 2014)</small></h2>
	    <table id="nav" cellpadding="0" cellspacing="8" width="120" >
	      <tr><td><a href="description.html">Description</a></td></tr>
	      <tr><td><a href="contenu.html">Contenu</a></td></tr>
	      <tr><td  class="active"><a href="evaluations.html">Évaluations</a></td></tr>
	    </table>
	    <h2>Annonces</h2>
	    <!--
	    <b>[06/01/2014]</b><br>
	    Notes finales du cours disponibles.</b><br><br>
	    <b>[06/01/2014]</b><br>
	    Notes des projets disponibles.</b><br><br>
	    <b>[13/12/2013]</b><br>
	    Notes des présentations disponibles.</b><br><br>
	    <b>[14/11/2013]</b><br>
	    Notes du devoir 3 disponibles.</b><br><br>
	    <b>[23/10/2013]</b><br>
	    Notes du devoir 2 disponibles.</b><br><br>
	    <b>[07/10/2013]</b><br>
	    Notes du devoir 1 disponibles.</b><br><br>
	      -->
	    <b>[25/08/2014]</b><br>
	    Bienvenue au cours IFT 725!
	  </div>
	  
	  <!--
	  <div id="sidebarright">
	  </div>
	  -->

	  <div id="contentnorightbar">
	    <h2> Participation au forum (10 points)</h2> 
        Tout au long de
        la session, un forum de discussion est mis à la disposition
        des étudiants. À chaque semaine, l'étudiant devra y
        participer, en y publiant au moins un message. Ce message peut
        être une question liée à la matière de la semaine ou une réponse
        à la question d'un autre étudiant.
        <br><br>

        Un compte-rendu de la participation de chaque étudiant pour chaque
        semaine est disponible <a href="forum/participation_forum.txt">ici</a>.

	    <h2> Devoirs (devoirs 1 et 2 = 14 points, devoir 3 = 18 points)</h2>

	    <p> Pour toute question concernant les devoirs, n'hésitez pas à poser une question
	    sur le <a href="https://groups.google.com/forum/?fromgroups#!forum/ift-725-a2014">forum de discussion</a> du cours!
	      </p>

	    <table cellspacing="0" >
	      <tr bgcolor="#FD9842">
		<td width="10%"><b>Devoir</b></td> 
		<td width="60%"><b>Thème</b></td>
		<td width="15%"><b>Date de remise</b></td>
		<td width="15% align="right""><b>Notes</b></td>
	      </tr>
	      
	      <tr>
		<td >1</td> 
		<td><a href="devoir_1/devoir_1.pdf">Réseau de neurones à propagation avant</a> [<a href="devoir_1/devoir_1_en.pdf">EN</a>]<br><br>
          Fichiers nécessaires:
          <ul>
            <li> <a href="devoir_1/nnet.py">nnet.py</a><br>
            <li> <a href="devoir_1/run_nnet.py">run_nnet.py</a><br>
            <li> <a href="devoir_1/run_verify_gradients.py">run_verify_gradients.py</a>
          </ul><br>
        </td>
		<td>26 septembre</td>
        <td>[<a href="devoir_1/notes.txt">txt</a>]</td>
	      </tr>
	      
	      <tr bgcolor="#EEE">
		<td >2</td> 
		<td><a href="devoir_2/devoir_2.pdf">Champs markovien conditionnel</a> [<a href="devoir_2/devoir_2_en.pdf">EN</a>]<br><br>
          Fichiers nécessaires:
          <ul>
            <li> <a href="devoir_2/crf.py">crf.py</a><br>
            <li> <a href="devoir_2/run_crf.py">run_crf.py</a><br>
            <li> <a href="devoir_2/run_verify_gradients.py">run_verify_gradients.py</a>
            <li> <a href="devoir_2/ocr_letters_sequential.py">ocr_letters_sequential.py</a>
            <li> <a href="devoir_2/download_ocr_letters_sequential.py">download_ocr_letters_sequential.py</a>
          </ul><br>
        </td>
		<td>22 octobre </td> 
        <td>[<a href="devoir_2/notes.txt">txt</a>]</td>
	      </tr>
	      
	      <tr               >
		<td >3</td> 
		<td><a href="devoir_3/devoir_3.pdf">Machine de Boltzmann restreintes, autoencodeurs et réseaux profonds</a> [<a href="devoir_3/devoir_3_en.pdf">EN</a>]<br><br>
          Fichiers nécessaires:
          <ul>
            <li> <a href="devoir_3/nnet.py">nnet.py</a><br>
            <li> <a href="devoir_3/rbm.py">rbm.py</a><br>
            <li> <a href="devoir_3/run_show_filters_rbm.py">run_show_filters_rbm.py</a><br>
            <li> <a href="devoir_3/rbm_filters.pdf">rbm_filters.pdf</a>
            <li> <a href="devoir_3/run_stacked_rbms_nnet.py">run_stacked_rbms_nnet.py</a><br>
            <li> <a href="devoir_3/autoencoder.py">autoencoder.py</a><br>
            <li> <a href="devoir_3/run_show_filters_autoencoder.py">run_show_filters_autoencoder.py</a><br>
            <li> <a href="devoir_3/autoencoder_filters.pdf">autoencoder_filters.pdf</a>
            <li> <a href="devoir_3/run_stacked_autoencoders_nnet.py">run_stacked_autoencoders_nnet.py</a><br>
            <li> <a href="devoir_1/rapport.pdf">exemple de rapport</a>
          </ul><br>
        </td>
		<td>5 novembre </td>
        <td>[<a href="devoir_3/notes.txt">txt</a>] </td>
	      </tr>
	    </table>
	    
	    <h2> Projet d'application d'un réseau de neurones (30 points)</h2>

	    En plus des 3 devoirs, l'étudiant doit également proposer
	    et accomplir un projet d'application d'un réseau de
	    neurones.  
	    La définition du projet doit être faite en concertation
	    avec le professeur. Un exemple de
	    projet pourrait être la reproduction de certains résultats
	    d'un article scientifique.  Un autre exemple serait
	    l'application d'un réseau de neurones à un problème lié au
	    thème de recherche de l'étudiant. 
	    <br><br>
	    Une proposition de projet 
	    répondant aux questions suivantes doit d'abord être remise:
	    <br><br>
	    <ul>
	      <li> Quel réseau de neurones sera utilisé (inclure
		une courte description ou un référence vers un article
		scientifique) ?
          <li> Qu'est-ce qui sera implémenté par l'étudiant et quel
            code sera plutôt emprunté d'ailleurs (par exemple du code obtenu du web) ?
	      <li> Quelles expériences seront exécutées et avec quelle
            méthode de référence (<i>baseline</i>) seront faites les
            comparaisons (la méthode de référence peut être une méthode
            très simple) ?
	    </ul>
	    <br>
	    La proposition de projet 
	    doit être remise <b>au plus tard le 12 novembre.</b>
	    <br><br>
	    Le projet devra également faire l'objet d'un rapport final,
	    devant être <b>remis au plus tard le 19 décembre</b>. Le
	    rapport devra présenter l'approche implémentée ainsi
	    que les résultats obtenus. Dans la mesure du possible, le projet doit être implémenté
	    en Python et utiliser la libraire 
	    <a href="http://info.usherbrooke.ca/hlarochelle/mlpython/">MLPython</a>.
	    Le code utilisé devra être remis.
	    <br><br>
        Plus de détails sur la grille d'évaluation qui sera utilisée lors
        de la correction sont disponibles <a href="projet/projet.pdf">ici</a> [<a href="projet/projet_en.pdf">EN</a>].
        <br><br>

        <b>Les notes des projets sont <a href="projet/notes.txt">ici</a>.</b>

        <br><br>
	    <h2> Présentation d'un article scientifique (14 points)</h2>

	    À la fin de la session, chaque étudiant doit faire une
	    présentation orale suite à la lecture d'un article scientifique
	    lié aux réseaux de neurones.
	    La présentation doit durer 20 minutes et sera suivie d'une
	    période de questions de 5 minutes. 
	    <br><br>
	    
	    L'étudiant peut choisir lui-même l'article à présenter (en
	    concertation avec le professeur) ou il peut le choisir
	    parmi les lectures suggérées de la section <b>Contenu</b> 
	    ou encore parmi les articles suivants:
	    
	    <br><br>
	    <ul id="contenucours">
	      <li> <a href="http://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"><i>Playing Atari with Deep Reinforcement Learning</i></a> 
		de <a href="http://www.cs.toronto.edu/~vmnih/">Volodymyr Mnih</a>, <a href="http://koray.kavukcuoglu.org/">Koray Kavukcuoglu</a>, <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Home.html">David Silver</a>, <a href="http://www.cs.toronto.edu/~graves/">Alex Graves</a>, Ioannis Antonoglou, <a href="http://people.idsia.ch/~daan/">Daan Wierstra</a> and <a href="http://ml.informatik.uni-freiburg.de/people/riedmiller/info">Martin Riedmiller</a>
	      <li> <a href="http://arxiv.org/pdf/1312.4400v3.pdf"><i>Network In Network</i></a> 
		de Min Lin, Qiang Chen et Shuicheng Yan
	      <li> <a href="http://www.cs.toronto.edu/~rsalakhu/papers/mnlm2014.pdf"><i>Multimodal Neural Language Models</i></a> 
		de <a href="http://www.cs.toronto.edu/~rkiros/">Ryan Kiros</a>, 
		<a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a> 
		et <a href="http://www.cs.toronto.edu/~zemel/inquiry/home.php">Richard Zemel</a>
	      <li> <a href="http://www.cs.toronto.edu/~rsalakhu/papers/Multimodal_DBM.pdf"><i>Multimodal Learning with Deep Boltzmann Machines</i></a> 
		de <a href="http://www.cs.toronto.edu/~nitish/">Nitish Srivastava</a> et <a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a>
	      <li> <a href="http://homepage.tudelft.nl/19j49/Publications_files/sample2e.pdf"><i>Hidden-Unit Conditional Random Fields
</i></a> 
		de <a href="http://homepage.tudelft.nl/19j49/Home.html">Laurens van der Maaten</a>, <a href="http://www.ics.uci.edu/~welling/">Max Welling</a> et <a href="">Lawrence K. Saul</a>
	      <li> <a href="http://gregoire.montavon.name/publications/montavon-lncs12.pdf"><i>Deep Boltzmann Machines and the Centering Trick</i></a> 
		de <a href="http://gregoire.montavon.name/">Grégoire Montavon</a> et <a href="http://www.ml.tu-berlin.de/menue/members/klaus-robert_mueller/">Klaus-Robert Müller</a>
	      <li> <a href="http://arxiv.org/pdf/1312.6114.pdf"><i>Auto-Encoding Variational Bayes</i></a> 
		de <a href="http://dpkingma.com/">Diederik P. Kingma</a> et <a href="http://www.ics.uci.edu/~welling/">Max Welling</a>
	      <li> <a href="http://arxiv.org/pdf/1212.5701.pdf"><i>ADADELTA: An Adaptive Learning Rate Method</i></a> 
		de <a href="http://www.matthewzeiler.com/">Matthew D. Zeiler</a>
	      <li> <a href="http://arxiv.org/pdf/1312.6199v4.pdf"><i>Intriguing properties of neural networks</i></a> de
		<a href="http://research.google.com/pubs/ChristianSzegedy.html">Christian Szegedy</a>, <a href="http://cs.nyu.edu/~zaremba/">Wojciech Zaremba</a>, <a href="http://www.cs.utoronto.ca/~ilya">Ilya Sutskever</a>, <a href="http://cims.nyu.edu/~bruna/Home.html">Joan Bruna</a>, <a href="http://www.dumitru.ca/">Dumitru Erhan</a>, <a href="http://www-etud.iro.umontreal.ca/~goodfeli/">Ian Goodfellow</a> et <a href="http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php">Rob Fergus</a>
	      <li> <a href="http://info.usherbrooke.ca/hlarochelle/publications/aistats2011_nade.pdf">
		  <i>The Neural Autoregressive Distribution Estimator</i></a> de
		<a href="http://info.usherbrooke.ca/hlarochelle/">Hugo Larochelle</a> et
		<a href="http://homepages.inf.ed.ac.uk/imurray2/">Iain Murray</a> [<a href="http://videolectures.net/aistats2011_larochelle_neural/">video</a>]
	      <li> <a href="http://books.nips.cc/papers/files/nips24/NIPS2011_1240.pdf">
		  <i>The Manifold Tangent Classifier</i></a>
		de <a href="http://www-etud.iro.umontreal.ca/~rifaisal/">Salah Rifai</a>,
		<a href="http://ynd.github.com/">Yann Dauphin</a>,
		<a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a>,
		<a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		et Xavier Muller [<a href="http://videolectures.net/nips2011_dauphin_manifold/">video</a>]
	      <li> <a href="http://www6.in.tum.de/Main/Publications/Graves2006a.pdf">
		  <i>Connectionist Temporal Classification: Labelling Unsegmented
		    Sequence Data with Recurrent Neural Networks</i></a> de
		<a href="http://www6.in.tum.de/Main/Graves">Alex Graves</a>,
		<a href="http://www.idsia.ch/~santiago/publications.html">Santiago Fernández</a>,
		<a href="http://www.idsia.ch/~tino/">Faustino Gomez</a>
		et <a href="http://www.idsia.ch/~juergen/">Jürgen Schmidhuber</a>
	      <li> <a href="http://www.uoguelph.ca/~gwtaylor/publications/jmlr2011/taylor11a.pdf">
		  <i>Two Distributed-State Models For Generating High-Dimensional Time Series</i></a>
		de <a href="http://www.uoguelph.ca/~gwtaylor/">Graham Taylor</a>,
		<a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> et
		<a href="http://www.cs.nyu.edu/~roweis/">Sam Roweis</a>
	      <li> <a href="http://www.uoguelph.ca/~gwtaylor/publications/cvpr2010/gwtaylor_cvpr2010.pdf">
		  <i>Dynamical Binary Latent Variable Models for 3D Human Pose Tracking</i></a>
		de <a href="http://www.uoguelph.ca/~gwtaylor/">Graham Taylor</a>,
		<a href="http://www.cs.brown.edu/~ls/">Leonid Sigal</a>,
		<a href="http://www.cs.toronto.edu/~fleet/">David Fleet</a> et
		<a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>
	      <li> <a href="http://research.microsoft.com/pubs/153169/cd-dnn-hmm-swb-interspeech2011-pub.pdf"><i>
		    Conversational Speech Transcription Using Context-Dependent Deep Neural Networks</i></a>
		de <a href="http://research.microsoft.com/en-us/people/fseide/">Frank Seide</a>,
		Gang Li et 
		<a href="http://research.microsoft.com/en-us/people/dongyu/">Dong Yu</a> [<a href="http://techtalks.tv/talks/conversational-speech-transcription-using-context-dependent-deep-neural-networks/57462/">video</a>]
	      <li> <a href="http://ai.stanford.edu/~quocle/faces_full.pdf"><i>
		    Building high-level features using large scale unsupervised learning</i></a>
		de <a href="http://ai.stanford.edu/~quocle/">Quoc Le</a>,
		<a href="http://www.cs.toronto.edu/~ranzato/">Marc'Aurelio Ranzato</a>,
		Rajat Monga,
		Matthieu Devin,
		Kai Chen,
		Greg Corrado,
		<a href="http://research.google.com/people/jeff/">Jeff Dean</a>
		et <a href="http://ai.stanford.edu/~ang/">Andrew Ng</a> [<a href="http://techtalks.tv/talks/building-high-level-features-using-large-scale-unsupervised-learning/57421/">video</a>]
	      <li> <a href="http://arxiv.org/pdf/1206.6392.pdf"><i>
		    Modeling Temporal Dependencies in High-Dimensional Sequences: 
		    Application to Polyphonic Music Generation and Transcription</i></a>
		de Nicolas Boulanger-Lewandowski,
		<a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>
		et <a href="http://www.iro.umontreal.ca/~vincentp/">Pascal Vincent</a> [<a href="http://techtalks.tv/talks/modeling-temporal-dependencies-in-high-dimensional-sequences-application-to-polyphonic-music-generat../57449/">video</a>]
	      <li> <a href="http://www.utstat.toronto.edu/~rsalakhu/papers/nonlinnca.pdf">
		  <i>Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure</i></a>
		de <a href="http://www.utstat.toronto.edu/~rsalakhu">Ruslan Salakhutdinov</a>
			  et <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a>
	    </ul>
	    <br>

	    L'étudiant doit confirmer le choix de son article
	    auprès du professeur <b>avant le 19 novembre</b>.<br><br>

        <b>Les notes des présentations sont <a href="presentation/notes.txt">ici</a>.</b>

	<h2> Notes finales </h2>
	    
	    Les <a href="notes_finales.txt">notes finales</a> du cours
	    sont maintenant disponibles.
	  </div>
	  
	  <div id="footer">
	  </div>
	  
	</div>
      </div>


    </body>
  </html>
