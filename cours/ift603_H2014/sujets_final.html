<!DOCTYPE html >
<html >
  <head>
    <title>Hugo Larochelle</title>
    <meta http-equiv="content-type" content="text/html; charset=iso-8859-1" />
    <link rel="stylesheet"
          href="http://fonts.googleapis.com/css?family=Droid+Sans:regular,bold"
          type="text/css" />
    <link rel="stylesheet" href="../../css/2.css" type="text/css" media="screen,projection" />

  </head>
  
  <body>

    <div id="wrapper">
      <div id="innerwrapper">

	<div id="header">
      <div id="header-text">
	    Hugo Larochelle
	  <!-- <h1 id="header-right"></h1>-->
	    <ul id="nav">
	      
	      <li><a href="../../index_fr.html" accesskey="a"><em>A</em>ccueil</a></li>
	      
	      <li><a href="../../publications_fr.html" accesskey="p"><em>P</em>ublications</a></li>

	      <li><a href="../../university_fr.html" accesskey="u" class="active"><em>U</em>niversité</a></li>

	      <li><a href="../../links_fr.html" accesskey="l"><em>L</em>iens</a></li>

	    </ul>
	  </div>
	  </div>
	  <div id="contentnobars">
	    <h2> Contenu du cours </h2>
	    
            <p>
              Voici la liste des sujets et concepts qui pourront faire l'objet de questions
	      dans l'examen final.
            </p>
	    
	    <p>
	      Pour toute question concernant ces sujets, n'hésitez pas à poser une question
	      sur le <a href="https://groups.google.com/forum/?fromgroups#!forum/ift-603-h2014">forum de discussion</a> du cours!
	    </p>
	    
	    <b>Tous les sujets avant l'intra</b><br>
	    Voir la liste <a href="sujets_intra.html">ici</a><br><br>

	    <b>Concepts fondamentaux (3)</b>
	    <ul>
	      <li> Définition des valeurs / vecteurs propres d'une matrice.
	      <li> Définition de la décomposition en valeurs / vecteurs propres d'une matrice symétrique.
	      <li> Lien entre les valeurs / vecteurs propres et le déterminant, la trace, le rang, l'inverse
		et la propriété "définie positive" d'une matrice.
	    </ul><br>
	    
	    <b> Formulation probabiliste (2,3)</b>
	    <ul> 
	      <li> Lien entre les valeurs / vecteurs propres d'une matrice de covariance et
	      la forme de la fonction de densité d'une loi gaussienne.
	      <li> Obtention de la loi marginale d'une gaussienne.
	      <li> Calcul de la loi conditionnelle d'une gaussienne.
	    </ul><br>

	    <b>Mélange de gaussiennes</b>
	    <ul>
	    <li> Définition du modèle de mélange de gaussiennes
	    <li> Explication de comment on suppose que les données ont été générées dans un mélange de gaussiennes.
	    <li> Lien entre le mélange de gaussiennes et l'approche probabiliste générative en classification.
	    <li> Description du problème de partitionnement en général.
	    <li> Utilisation d'un mélange de gaussiennes pour le partitionnement.
	    <li> Probabilité marginale de l'entrée dans un mélange de gaussiennes. 
	    <li> Description des étapes E et M pour un mélange de gaussiennes.
	    <li> Description générale du fonctionnement de l'algorithme EM, dans sa
	      façon d'optimiser la log-vraisemblance des données.
	    <li> Lien entre les hyper-paramètres du mélange de gaussiennes et sa capacité.
	    <li> Problèmes possibles avec l'algorithme EM (optima locaux, problème mal défini).
	    <li> Définition de la fenêtre de Parzen.
	    </ul><br>
	    
	    <b> Réduction de dimensionnalité</b>
	    <ul>
	    <li> Définition du problème de la réduction de dimensionnalité.
	    <li> Motivation pour faire de la réduction de dimensionnalité (visualisation de données, combattre la malédiction de la dimensionnalité).
	    <li> Description des concepts de dimensionnalité intrinsèque et de variété.
	    <li> Calcul d'une réduction de dimensionnalité basée sur l'ACP (versions de base, centrée ou centrée et normalisée).
	    <li> Lien entre les valeurs / vecteurs propres de la matrice de covariance et la variance d'une projection.
	    <li> Calcul de la reconstruction (ou décompression) d'un vecteur sous l'ACP.
	    <li> Lien entre les valeurs propres de la matrice de covariance et l'erreur de reconstruction.
	    <li> Description des étapes de l'ACP à noyau.
	    <li> Calcul du centrage d'un noyau.
	    <li> Sélection d'hyper-paramètres pour la réduction de dimensionnalité.
	    </ul><br>
	    
	    <b> Combinaison de modèles</b>
	    <ul>
	    <li> Description de la combinaison de différents algorithmes d'apprentissage (en régression et en classification).
	    <li> Description du <i>bagging</i>.
	    <li> Description de la génération d'un ensemble de données <i>bootstrap</i>.
	    <li> Description générale des propriétés du <i>bagging</i> (condition pour que le <i>bagging</i> aide de façon optimale).
	    <li> Description général du concept du <i>boosting</i>.
	    <li> Calcul des étapes de l'algorithme AdaBoost.
	    <li> Identification de la perte optimisée par AdaBoost et description générale de comment AdaBoost l'optimise.
	    <li> Distinction entre les cas où le <i>bagging</i> est plus prometteur
	      de ceux où le <i>boosting</i> est plus approprié.
	    </ul><br>
	    
	    <b> Apprentissage bayésien</b>
	    <ul>
	    <li> Description générale de l'approche par apprentissage bayésien et lien avec
	      les concepts de :
	      <ul>
		<li> Probabilités a priori sur les modèles.
		<li> Probabilités des données étant donné un modèle.
		<li> Probabilités a posteriori des modèles.
	      </ul>
	    <li> Avantages de l'apprentissage bayésien.
	    <li> Définition de la régression linéaire bayésienne.
	    <li> Progression des probabilités a posteriori des modéles lorsque
	      la quantité de données augmente.
	    <li> Définition générale de la loi prédicitve a posteriori.
	    <li> Propriété d'un processus gaussien.
	    <li> Prédiction et variance prédictive de la régression basée sur un processus gaussien.
	    </ul><br>
	    
	    <b> Méthodes d'échantionnage</b>
	    <ul>
	    <li> Échantillonnage de variables discètes et continues discrètes
	    <li> Échantillonnage de variables gaussiennes vectorielles
	    <li> Échantillonnage de variables aléatoires gaussiennes multi-dimensionnelles.
	    <li> Définition et propriétés de l'échantillonnage par rejet.
	    <li> Définition et propriétés de l'échantillonnage préférentiel.
	    <li> Description générale du MCMC.
	    <li> Description de l'algorithme de Metropolis-Hastings.
	    <li> Description de l'algorithme de l'échantillonnage de Gibbs.
	    </ul><br>

	  </div>
	  
	  <div id="footer">
	  </div>
	  
	</div>
      </div>


    </body>
  </html>
