<!DOCTYPE html >
<html >
  <head>
    <title>Hugo Larochelle</title>
    <meta http-equiv="content-type" content="text/html; charset=iso-8859-1" />
    <link rel="stylesheet"
          href="http://fonts.googleapis.com/css?family=Droid+Sans:regular,bold"
          type="text/css" />
    <link rel="stylesheet" href="../../css/2.css" type="text/css" media="screen,projection" />

  </head>
  
  <body>

    <div id="wrapper">
      <div id="innerwrapper">

	<div id="header">
      <div id="header-text">
	    Hugo Larochelle
	  <!-- <h1 id="header-right"></h1>-->
	    <ul id="nav">
	      
	      <li><a href="../../index_fr.html" accesskey="a"><em>A</em>ccueil</a></li>
	      
	      <li><a href="../../publications_fr.html" accesskey="p"><em>P</em>ublications</a></li>

	      <li><a href="../../university_fr.html" accesskey="u" class="active"><em>U</em>niversité</a></li>

	      <li><a href="../../links_fr.html" accesskey="l"><em>L</em>iens</a></li>

	    </ul>
	  </div>
	  </div>
	  <div id="contentnobars">
	    <h2> Contenu du cours </h2>
	    
            <p>
              Voici la liste des sujets et concepts qui pourront faire l'objet de questions
	      dans l'examen intra.
            </p>
	    
	    <p>
	      Pour toute question concernant ces sujets, n'hésitez pas à poser une question
	      sur le <a href="https://groups.google.com/forum/?fromgroups#!forum/ift-603-h2014">forum de discussion</a> du cours!
	    </p>
	    
	    <b> Concepts fondamentaux</b>
	    <ul>
	    <li> Qu'est-ce qu'un algorithme d'apprentissage ?
	    <li> Qu'est-ce que la généralisation ?
	    <li> Notions d'entrée, cible, exemple, ensemble d'entraînement et ensemble de test.
	    <li> Qu'est-ce qu'un modèle ?
	    <li> Apprentissage supervisé vs. non-supervisé.
	    <li> Classification vs. régression.
	    <li> Fonction de perte (erreur, coût).
	    <li> Qu'est-ce que le sous-apprentissage ? Qu'est-ce que le sur-apprentissage ?
	    <li> Qu'est-ce que la capacité d'un algorithme ou modèle ?
	    <li> Relation entre la capacité et l'erreur d'entraînement vs. de généralisation.
	    <li> Relation entre la quantitié de données d'entraînement et l'erreur d'entraînement vs. de généralisation.
	    <li> Qu'est-ce que la régularisation (telle la régularisation de la norme Euclidienne au carré) ?
	    <li> Qu'est-ce qu'un hyper-paramètre ?
	    <li> Qu'est-ce que la sélection de modèle ?
	    <li> Comment on utilise un ensemble de validation pour faire de la sélection de modèle.
	    <li> Qu'est-ce que la <i>S-fold cross-validation</i>.
	    <li> Qu'est-ce que la recherche sur grille.
	    <li> Qu'est-ce que la malédiction de la dimensionnalité ? 
	    </ul><br>
	    
	    <b> Formulation probabiliste</b>
	    <ul>
	    <li> Variables aléatoires continues: fonction de densité (jointe, marginale, conditionnelle), fonction de répartition.
	    <li> Variables aléatoires multidimensionnelles.
	    <li> Calcul et propriétés de l'espérance, de la variance et de la matrice de covariance.
	    <li> Définition et propriétés de la loi gaussienne, lien entre la forme de la fonction de densité et ses paramètres.
	    <li> Hypothèse i.i.d.
	    <li> Principe du maximum de vraisemblance.
	    <li> Principe du maximum a posteriori (loi a priori, loi a posteriori)
	    <li> Lien entre le maximum a posteriori et l'utilisation de régularisation.
	    <li> Entropie, information, loi d'entropie maximale vs. minimale, entropie conditionnelle, entropie différentielle.
	    <li> Divergence de Kullback-Leibler.
	    <li> Information mutuelle.
	    </ul><br>
	    
	    <b> Régression linéaire</b>
	    <ul>
	    <li> Modèle de régression linéaire et lien entre ses paramètres et la forme du modèle.
	    <li> Utilité des fonctions de base, fonctions de base polynomiales et gaussiennes.
	    <li> Hypothèses probabilistes de la régression linéaire. 
	    <li> Solution du maximum de vraisemblance en régression.
	    <li> Solution du maximum a posteriori en régression.
	    <li> Hypothèses probabilistes du maximum de vraisemblance en prédictions multiples.
	    <li> Principe général de la théorie de la décision.
	    <li> Liens entre la notion de capacité et les notions de biais et de variance.
	    <li> Lien entre l'hyper-paramètre de la régression et la capacité.
	    </ul><br>
	    
	    <b> Classification linéaire</b>
	    <ul>
	    <li> Notions de régions de décision, surfaces de décision, fonction discriminante.
	    <li> Classification binaire vs. à multiples classes.
	    <li> Qu'est-ce qu'un problème linéairement séparable ? 
	    <li> Théorème sur la séparabilité linéaire.
	    <li> Paramétrisation des modèles de classification linéaire et lien avec la forme de la surface de décision.
	    <li> Calcul de la distance avec la surface de décision.
	    <li> Pourquoi existe-t-il plusieurs algorithmes de classification linéaire ?
	    <li> Description de l'algorithme des moindres carrés.
	    <li> Principe derrière l'analyse discriminante linéaire et son lien avec les notions de variances intra-classe et inter-classe.
	    <li> Hypothèses probabilistes derrière l'approche probabiliste générative.
	    <li> Solution du maximum de vraisemblance et formulation sous la forme d'un classifieur linéaire.
	    <li> Règle de Bayes.
	    <li> Hypothèses probabilistes derrière l'approche probabiliste discriminante (régression logistique).
	    <li> Cross-entropie binaire.
	    <li> Algorithme de la descente de gradient.
	    <li> Comment convertir la classification à multiples classes en problèmes de classification binaire ?
	    <li> Lien entre les hyper-paramètres des algorithmes de classification et la capacité.
	    </ul><br>
	    
	    <b> Formulation probabiliste (2)</b>
	    <ul>
	    <li> Prétraitement de données catégoriques, vecteur <i>one-hot</i>.
	    <li> Adaptation au cas de données manquantes.
	    <li> Normalisation de données réelles.
	    <li> Calcul d'intervalles de confiance.
	    </ul><br>

	    <b> Méthodes à noyau</b>
	    <ul>
	    <li> Motivation derrière la formualtion d'un algorithme sous une représentation duale.
	    <li> Notions de noyau et matrice de Gram.
	    <li> Formulation à noyau de la régression (entraînement et prédiction).
	    <li> Description de l'astuce du noyau et de son utilité.
	    <li> Définition d'un noyau valide et construction de noyaux valides.
	    <li> Noyaux polynomial et gaussien, lien entre leurs hyper-paramètres et la capacité du modèle à noyau.
	    </ul><br>
	    
	    <b> Machine à vecteurs de support (SVM)</b>
	    <ul>
	    <li> Avantage du SVM par rapport à d'autres méthodes à noyau.
	    <li> Notion de marge et de distance signée avec la surface de décision.
	    <li> Principe de la classification à marge maximale.
	    <li> Formulation de l'entraînement d'un SVM par programmation quadratique.
	    <li> Notion de vecteurs de support et lien avec la représentation duale du SVM.
	    <li> Utilisation de variables de ressort pour le traitement du chevauchement de classes et programme quadratique adapté.
	    <li> Lien entre la <i> hinge loss</i> et les variables de ressort.
	    <li> Différence entre les pertes de différents algorithmes de classification.
	    <li> Lien entre les hyper-paramètres du SVM et la capacité.
	    </ul><br>
	    

	  </div>
	  
	  <div id="footer">
	  </div>
	  
	</div>
      </div>


    </body>
  </html>
